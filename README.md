# CNPJ ETL

> [!NOTE]  
> This idea was inspired by a [https://www.linkedin.com/feed/update/urn:li:activity:7408660461782618112?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7408660461782618112%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29](LinkedIn post) that provides valuable resources for hands-on practice of data engineering concepts.

**The Brazilian Federal Revenue Service (Receita Federal)** provides free public data on **50+ million Brazilian companies (CNPJs)**. These are **tens of GBs of CSV files** that must be:

âœ“ Downloaded and processed monthly
âœ“ Normalized (CNAE codes, municipalities, legal nature)
âœ“ Related (companies + establishments + partners/shareholders)
âœ“ Transformed into value (aggregations, dataset joins, data cleaning, etc.)

**Data link:** [https://lnkd.in/eK3QGc9Q](https://lnkd.in/eK3QGc9Q)

---

## **THE CHALLENGE (choose your level)**

### ðŸŸ¢ **BEGINNER**

Build a script that downloads and loads the data into PostgreSQL/MySQL. Answer using SQL:

âœ“ How many active companies per state?
âœ“ Top 10 most common CNAE codes
âœ“ Distribution by company size (MEI, ME, EPP)

**Suggested skills:** Python, SQL, Pandas

### ðŸŸ¡ **INTERMEDIATE**

Create an automated ETL pipeline:

âœ“ Scheduled incremental downloads
âœ“ Data integrity and duplication validation
âœ“ Dimensional modeling (star schema)
âœ“ Basic dashboard in Metabase / Superset / Power BI / Tableau

**Suggested skills:** Airflow, dbt, DVC, Docker, AWS S3, AWS RDS

### ðŸ”´ **ADVANCED**

Build a complete data engineering architecture:

âœ“ Distributed ingestion (Spark / Dask / Trino)
âœ“ Data lake built with Medallion architecture
âœ“ Metadata catalog for documentation
âœ“ REST API for queries (you could even charge for this!)
âœ“ Monitoring and data quality
âœ“ Full CI/CD (data deployed to production)

**Suggested skills:** Spark, Kubernetes, Terraform, FastAPI, Great Expectations, AWS API Gateway
